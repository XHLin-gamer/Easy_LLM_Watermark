# LLM は次の単語（トークン）を予測する技術です


皆さん、言語を使っていますか？多分使っていない読者はいないだと思います。しかし、我々が日々使っている言語というのは、どうやって論理的に説明するのか、しっかり考えたことのある方がそんなに多くないだどう。例えば、以下のもののすべてが言語の事象になります：

---
我々は月に行くことを選択する

Why we choose to go to the moon

Мы решили отправиться на луну

വി ചൂസ് റ്റു ഗോ റ്റു ദി മൂൺ

---

これはかのケネディ大統領の有名の演説「我々は月に行くことを選択する」の、いろんの言語のバージョンです。上から日本語、英語、ロシア語と筆者でも知らない、ウィキペディアで適当に拾った見たことのない言語です。このように、人間にとっては、言語を理解するために、長い時間をかけて学習する必要性があり、学習したことのない符号や言語などを理解するのは到底できないです。それでは、パソコンに人間の言語を学習させるためには、どのような技術が必要としているでしょう。

## パソコンの単語：トークン

情報学科に詳しい方はすでに分かっているかもしれないが、答えはデータ化です、数字とも言えます。以上お見せ示した言語のすべてが、データとして、パソコンに保存しております。有名なのは、UTF８やJISコードなどのプロトコルで、誤ったら、文字化けが出で来るやつです。

![alt text](../pics/encoding.png)

＃　修正↑

上の図に示したのは、人間の言語をデータ（数字）になるとどうなるのかの一例です[1]。採用された文字コードがJIS、よくなじむ日本産業規格（JIS）に収録されているものです。日本産業規格にしても、別に日本語しか対応できないわけではなく、いろんな言語が対応できるということがはっきりわかります。其の上、言語の符号、それは漢字でもアルファベットでもよく知らない文字でも、全部数字に変換することが可能です。皆さん毎日使っているスマホやパソコンの中に、実際人間の言語ではなく、こういった数字で動かしているということは明白です。こういった文字コードはまさに、パソコンの言語で、数字一個一個がパソコンの単語のようなものに等しいでしょう。

それでは、このJIS標準を使って、そもままAIの訓練に用いられることは可能でしょうか？残念ながら、それはできません。またっくできないというわけではないが、著しく欠点が存在しております。それは、単語の量が多すぎるということです。例えば、実際の開発でよくつかわれるUnicodeで、収録可能の符号位置は111万4,112個あって、現在使われていた数だけでも１６万弱に達しています[3, 4]。もしこの符号がパソコン言語の単語で例えるなら、１６万の単語を保有する言語になります。もちろん、単語の数が小さいほど、言語が学習しやすいのは言うまでもなく、Unicodeのような文字コードをそのまま使うと、学習する難易度が結構高まることが予想されるでしょう。

それではどうするのだ、情報量を保持しつつ、単語の数を減らす方法は存在しますか？というと、存在しています。それは、バイト対符号化（ＢＰＥ）[2]のような、テキスト圧縮アルゴリズムです。皆さんがフォルダをＺＩＰファイルにするときに、別に記録されていました情報量が変更したわけではなく（元に戻せる）、しかし、ディスク容量は確実に節約することができます。ＢＰＥの名前は一見難しいそうですが、ふたを開けたら、そんなに難しいことをやっていない。ウィキペディアの例[5]を引用し、説明すると
![alt text](../pics/BPE.svg)

操作ごとに説明すると、一行目に、元の文は ```ABCDCDABCDCDE``` になります、総計アルファベット１３個があります。その中に、```AB``` のような対（ペア）が二回、```CD```のような対が四回現れました。こういった出現頻度が２以上の対を、新たな符号を与えます。例えば、```AB```を```Y```にし、元の文の中の```AB```の部分を全部```Y```に書き換えることにより、元の部分的な長さは二分の一にできます（もう一つの例として、もしある文```ABABAB```が存在して、```Y```を代入すると、```YYY```になるはずです。これで一気に半分くらい短縮することができました）。```{Y = AB, Z = CD}```を第一行目のテキストに代入すると、二行目のテキストが出できました。ここでさっきと同じ操作を行います、すなわちよく出で来る対を探します。第二行目では、よく出で来る対は```YZ```ということで、新たな```X <- YZ```として辞書に追加し、第三行のテキストが出できました。最終行のテキストの中に、唯一のついは```WW```ということで、しかも出現頻度は一回しかありません。ここで、アルゴリズム終了とします。

この例は一見いい感じに説明して、まとまっているところ、違和感を覚えている方は極めて鋭いです。

![alt text](../pics/BPE2.svg)

１３個アルファベットのテキストを三つのアルファベットに圧縮するのはいいか、忘れちゃいけないものがあります。そう、辞書そのものを容量をかかります。もし辞書の容量も計算すると、なんと、１３個のアルファベットを圧縮するために、15個のアルファベットを消費しました。これは逆に容量を増えているではないか？でも、別にこれがBPE技術が実は無効というわけではなく、あくまでも、ウィキペディアの例が誤解しやすいだけです。例えば、元のテキストが二倍長さになる場合はどうなりますか：

![alt text](../pics/BPE3.svg)

同じ辞書を使う場合、別に取り扱うテキストの長さが倍にしても、辞書の長さは不変であることは自明です。同時に、辞書化した後のテキストも元のテキストと同じ二倍にすることになります。しかし、辞書化した後に、二倍にした部分は極端に圧縮したことがものが語っていって、全体の容量は１８しか消費していません。この場合、26個のアルファベットを消費するはずの場合、辞書化した後18しかつかっていないという状況になりました。もし、テキストがもっと長くなると、こういった圧縮の効果がより顕著になるでしょう。

これで、一見落着になるですが、実はもう一つ問題が残っています。ウィキペディアの例の第四行目をよく見てください。辞書の中に、```W, X, Y, Z```が含まれていって、```E```がそのまままだ残っている。こういう辞書作りをするときに、漏らさないために、その辞書は理論的な最小サイズが存在しております。カッコ良く言い換えれば、「数学的な下限」が存在しております。でも、この最小サイズが実は結構小さいもので、例えば、「吾輩は猫である」の全部を符号化するために、128個の単語さえあれば十分です（別に単語の数が少ないほうがいいわけではありません、あくまでも人間の言語が案外に圧縮できるということの一例です）。それでは、もし、その辞書の中に存在していない単語が出できたら、どういうことが起こるのか。いうまでもなく、取り扱うできません。エラーが出できます。これが昔の言語モデルが、絵文字などのデータを処理するときに性能が悪いの一因にもなっています。

一端話をまとめると、より効率のいい文字コードを作るために、バイト対符号化（ＢＰＥ）がよくつかわれています。ＢＰＥとは、データを圧縮する技術であり、言語処理においては、数字化した人間の言語の中に、よく出で来るパターンをかき集め、パソコンの辞書を作ること。こういう人間の言語をいちいちパソコンの単語に変換するものは、みんながよく言う**Tokenizer**で、パソコンの単語の正式名称も**Token**となります。

それでは、実際にTokenizer体験してみよう。これから、少々コードが出ます、分かりやすいように書いているのは、ご心配なく。

とりあえず、Tokenizerを作るためには、元になるテキストが必要です。ここで、夏目漱石の「吾輩は猫である」を辞書を生み出すテキスト元とします。

```python
from 有用なスクリプト import load_txt
for sentence in load_txt(path="./吾輩は猫であるutf8.txt")[:10]:
    if len(sentence) > 1:
        print(sentence if len(sentence) < 50 else sentence[:50]+"　（以下略）")
```
そして出力は『吾輩は猫である』の一部です
```
吾輩は猫である

夏目漱石

　吾輩《わがはい》は猫である。名前はまだ無い。

　どこで生れたかとんと見当《けんとう》がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事　（以下略）
　この書生の掌の裏《うち》でしばらくはよい心持に坐っておったが、しばらくすると非常な速力で運転し始め　（以下略）
　ふと気が付いて見ると書生はいない。たくさんおった兄弟が一｜疋《ぴき》も見えぬ。肝心《かんじん》の母　（以下略）
　ようやくの思いで笹原を這い出すと向うに大きな池がある。吾輩は池の前に坐ってどうしたらよかろうと考え　（以下略）
　吾輩の主人は滅多《めった》に吾輩と顔を合せる事がない。職業は教師だそうだ。学校から帰ると終日書斎に　（以下略）
　吾輩がこの家へ住み込んだ当時は、主人以外のものにははなはだ不人望であった。どこへ行っても跳《は》ね　（以下略）
```
これはすでにGitHubで用意されているテキストファイルです。それでは、実際にTokenizerを作ってみよう。具体的な実装方法は割愛します。興味のある方は```有用なスクリプト.py```に参照してください。重要なことは、Tokenizerを訓練するために、必要な要素として、ただテキスト元とほとんど任意に指定できる辞書の大きさだけが必要なんです。
```python
from 有用なスクリプト import train_tokenizer

tokenizer = train_tokenizer(
    file_path="./吾輩は猫であるutf8.txt",   # テキスト元になるファイル
    vocab_size=5000,                       # 辞書の大きさ
    text_to_compress = "吾輩わがはいは猫である。名前はまだ無い。"
)
```
このコード出力結果として、以下のものが出で来るはずです：
```
Tokenizerの指定単語数は 5000...
訓練完了

--- テキスト容量比較 ---
比較用サンプル: '吾輩わがはいは猫である。名前はまだ無い。'

[UTF-8 容量]
  bytes: 60
  bits (bytes * 8): 480

[BPE 容量]
  辞書容量: 5000
  トークンごとのbits密度 (ceil(log2(vocab_size))): 13
  トークン数: 10
  トークン: [315, 2459, 535, 144, 4662, 142, 1873, 3784, 3646, 142]
  bits (トークン数 * bits_per_token): 130

--- 📊 最終結果 ---
UTF-8 Bits: 480
BPE Bits:   130
-> 圧縮比率: 3.69 (UTF-8 bits / BPE bits)
```
「吾輩わがはいは猫である。名前はまだ無い。」の一言を数字化するため、UTF８（Unicode）なら480 bitsを消費するに対してBPEが辞書の大きさが5000の状態で１３０　bitsしかかかりません。これは大きな圧縮です。

次に、このトークンが元のテキストに戻れるかどうかを試してみよ、人間の理解できる言語に戻せないなら、元も子もないからな：
```python
適当な文章 = "論文を書くのは辛い El Psy Congroo 🥺"
print("適当な文章:\t", 適当な文章)
トークン = tokenizer.encode(適当な文章).ids
print("トークン: \t", トークン[:15])
日本語文書 = tokenizer.decode(トークン)
print("復元した日本語:\t", 日本語文書)
```
出力として：
```
適当な文章:	 論文を書くのは辛い El Psy Congroo 🥺
トークン: 	 [4114, 3431, 158, 349, 2909, 132, 93, 36, 93, 42, 48, 93, 39, 38, 32]
復元した日本語:	 論文を書くのは辛い l sy ongroo ���
```
が出できました。これはバグではなく、前説明した通り、Tokenizerは与えたテキスト元の中に存在している単語しか処理できないため、夏目先生は絵文字や英語など使っていないから、代表作から生み出されたTokenizerも絵文字の処理能力がないのも当然です。それでは、夏目先生の作品以外に、いろいろデータを足しましょう：

```python

tokenizer = train_tokenizer(
    file_path=[
        "./吾輩は猫であるutf8.txt",
        "./text/emoji.txt",
        "./text/harry potter 1.txt"
    ],   # テキスト元になるファイル
    vocab_size=5000,                       # 辞書の大きさ
    text_to_compress = "吾輩わがはいは猫である。名前はまだ無い。"
)
print("----------------------------")
適当な文章 = "論文を書くのは辛い El Psy Congroo 🥺"
print("適当な文章:\t", 適当な文章)
トークン = tokenizer.encode(適当な文章).ids
print("トークン: \t", トークン[:15])
日本語文書 = tokenizer.decode(トークン)
print("復元した日本語:\t", 日本語文書)
```
そして
```

Tokenizerの指定単語数は 5000...
訓練完了

--- テキスト容量比較 ---
比較用サンプル: '吾輩わがはいは猫である。名前はまだ無い。'

[UTF-8 容量]
  bytes: 60
  bits (bytes * 8): 480

[BPE 容量]
  辞書容量: 5000
  トークンごとのbits密度 (ceil(log2(vocab_size))): 13
  トークン数: 13
  トークン: [460, 3696, 813, 167, 808, 324, 164, 2836, 167, 1158, 566, 151, 164]
  bits (トークン数 * bits_per_token): 169

--- 📊 最終結果 ---
UTF-8 Bits: 480
BPE Bits:   169
-> 圧縮比率: 2.84 (UTF-8 bits / BPE bits)
----------------------------
適当な文章:	 論文を書くのは辛い El Psy Congroo 🥺
トークン: 	 [988, 944, 346, 1700, 187, 511, 4384, 151, 112, 26, 51, 112, 32, 58, 64]
復元した日本語:	 論文を書くのは辛い El Psy ongroo 🥺

```
＃　要修正

絵文字が普通に処理できました。しかし、英語のデータを導入しても、「Congroo」という単語が復元できませんでした。理由は言うまでもなく、追加した英語データセットはハリーポッターので、その中に「Congroo」という単語が存在していないです。単語漏れの現象は自然言語においては、元のデータセットをバンバン充填したら、ある程度解決できます。しかし、よく最近のAIを使っている方はご存知のように、AIに画像を入力することもできます。画像を入力する際に、やはりトークンに変換する必要があり、その変換するの中にある程度の情報紛失は避けられない。興味のある方は付録に参照してください、本編では、主に自然言語だけを対象として進みます。

一言でこれまでの話をまとめると：Tokenizerという人間の言語をToken（数字）に変換するものがあって、それがLLMの重要な基礎となります。それでは、いよいよLLMの部分に入りますとしよう。


## 次の単語へ

言語モデルについて話す前に、とりあえず簡単なウォーミングアップをしよう：

```python
パターン１ = [1, 2, 3, 4, 5, ...]
パターン２ = [1, 4, 9, 16, 25, ...]
パターン３ = [1, 2, 4, 8, 16, ...]
```

ここに三つの数列があって、それそれはは五つ数も知っています。それでは、皆さん、この三つのパターンの次にどの数字が出ますかを当ててみてください。何だこりゃ、と思っている方もおられるかもしれないが、確かし、これは幼稚園レベルの問題で、パターン１はシンプルな増加列、パターン２は完全平方数の増加列で、最後のは２の指数の数列です。

しかし、よくよく考えたら、これは変に思わないですか。なんで無限の可能性のある数列から、たった五つの数字だけれ、数列全体のパータンを決めつけるのですか。そう思われている方は鋭いです。こういう問題の暗黙の了解として、数列が何らかしらのルールに乗っ取っていて、そして、そのルールというものが有限の観察から推理できるということです。

しかし、こういう観察は完備な証明と違い、果たして推理したルールが全体に当てはまるかどうかは根本的に担保することができません。例えば、もしパターン１実はこういうようなものとして
```python
def パターン１(x: int):
    # ｘが１から５の時、ｘしか値として戻っていない；
    return x + (x - 1) * (x - 2) * (x - 3) * (x - 4) * (x - 5) 

新パターン１ = [パターン1(x) for x in range(1, 7)]
print("新パターン１ ->", 新パターン1)
```
その結果は当然、
```
新パターン１ -> [1, 2, 3, 4, 5, 126]
```
になります。有限の観察から、絶対的なパータンは取得することができません。だが、もしその有限の観察がただ五つの数字ではなくで、何十億回何百億回の観察を積み重ねて、そこから得られる経験が五つの観察から得られた知見より、よっほど裏に存在している真のパータンに近づいているとも言えます。

ここで、言語モデルの話に戻るか、上に説明した通り、文書が数字化することができる。それでは、文書とTokenizerから数字化したTokenを並んでみましょう：
```python
文書 = "月がきれい"
for idx in range(0, len(文書)):
    文書断片 = 文書[: idx + 1]
    tokens = tokenizer.encode(文書断片)
    print("　" * (len(文書) - idx), 文書断片, ":", end=" ")
    print(tokens.ids)
```

```
　　　　 　月 : [493]
　　　　 月が : [493, 170]
　　　 月がき : [493, 170, 230]
　　 月がきれ : [493, 170, 230, 199]
　 月がきれい : [493, 170, 230, 199, 151]
```


[1]https://www.edu.i.hosei.ac.jp/~sigesada/kyouzai/mojicodehenkan.html

[2]Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019): 9.

[3]https://ja.wikipedia.org/wiki/Unicode

[4]https://www.unicode.org/versions/Unicode16.0.0/UnicodeStandard-16.0.pdf

[5]https://ja.wikipedia.org/wiki/バイト対符号化

[5]https://huggingface.co/learn/llm-course/chapter6/5#:~:text=For%20real%2Dworld,emojis%2C%20for%20instance.