# LLM は次の単語（トークン）を予測する技術です


皆さん、言語を使っていますか？多分使っていない読者はいないだと思います。しかし、我々が使っている言語というのは、どういうことなのか、しっかり考えたことのある人がそんなに多くないだどう。以下のもののすべてが言語の事象の一つになります：

---
我々は月に行くことを選択する

Why we choose to go to the moon

Мы решили отправиться на луну

വി ചൂസ് റ്റു ഗോ റ്റു ദി മൂൺ

---

これはかのケネディ大統領の有名の演説「我々は月に行くことを選択する」のいろんの言語のバージョンです。上から日本語、英語、ロシア語と筆者でも知らない、ウィキペディアで適当に拾った見たことのない言語です。このように、人間にとっては、言語を理解するために、長い時間をかけて学習する必要性があり、学習したことのない符号や言語などを理解するのは到底できない技です。それでは、パソコンにとって、人間の言語は何ですか？


情報学科に詳しい方はすでに分かっているかもしれないが、答えはデータです、数字とも言えます。以上お見せ示した言語のすべてが、データとして、パソコンに保存しております。一番有名なのは、UTF８やJISコードなどのプロトコルで、誤ったら、文字化けが出で来るやつです。

![alt text](../pics/encoding.png)

＃　修正↑

上の図に示したのは、人間の言語をデータ（数字）になるとどうなるのかの一例です[1]。採用された文字コードがJIS、よくなじむ日本産業規格（JIS）に収録されているものです。日本産業規格にしても、別に日本語しか対応できないわけではなく、いろんな言語が対応できるということがはっきりわかります。其の上、言語の符号、それは漢字でもアルファベットでもよく知らない文字でも、全部数字に変換することが可能です。皆さん毎日使っているスマホやパソコンの中に、実際人間の言語ではなく、こういった数字で動かしているということは明白です。こういった文字コードプロトコルはまさに、パソコンの言語で、数字一個一個がパソコンの単語のようなものに等しいでしょう。

それでは、このJIS標準を使って、そもままAIの訓練に用いられることは可能でしょうか？残念ながら、それはできません。またっくできないというわけではないが、著しく欠点が存在しております。それは、単語の量が多すぎるということです。例えば、実際の開発でよくつかわれるUnicodeで、収録可能の符号位置は111万4,112個あって、現在使われていた数は１６万弱です[3, 4]。もしこの符号がパソコン言語の単語で例えるなら、１６万くらいの単語が保有する言語になります。もちろん、単語の数が小さいほど、言語が学習しやすいのは言うまでもなく、Unicodeのような文字コードをそのまま使うと、学習する難易度が結構高まることになります。

それではどうするのだ、情報量を保持しつつ、単語の数を減らす方法は存在しますか？というと、存在しています。それは、バイト対符号化（ＢＰＥ）[2]のような、圧縮アルゴリズムです。皆さんがフォルダをＺＩＰファイルにするときに、別に記録されていました情報量が変更したわけではなく（元に戻せる）、しかし、ディスク容量は確実に節約することができます。ＢＰＥの名前は一見難しいそうですが、ふたを開けたら、そんなに難しいことをやっていない。ウィキペディアの例[5]を引用し、説明すると
![alt text](../pics/BPE.svg)

操作ごとに説明すると、一行目に、元の文は ```ABCDCDABCDCDE``` になります、総計アルファベット１３個があります。その中に、```AB``` のような対（ペア）が二回、```CD```のような対が四回現れました。こういった出現頻度が２以上の対を、新たな符号を与えます。例えば、```AB```を```Y```にし、元の文の中の```AB```の部分を全部```Y```に書き換えることにより、元の部分的な長さは二分の一にできます（もう一つの例として、もしある文```ABABAB```が存在して、```Y```を代入すると、```YYY```になるはずです。これで一気に半分くらい短縮することができました）。```{Y = AB, Z = CD}```を第一行目のテキストに代入すると、二行目のテキストが出できました。ここでさっきと同じ操作を行います、すなわちよく出で来る対を探します。第二行目では、よく出で来る対は```YZ```ということで、新たな```X <- YZ```として辞書に追加し、第三行のテキストが出できました。最終行のテキストの中に、唯一のついは```WW```ということで、しかも出現頻度は一回しかありません。ここで、アルゴリズム終了とします。

この例は一見いい感じに説明して、まとまっているところ、違和感を覚えている方は極めて鋭いです。

![alt text](../pics/BPE2.svg)

１３個アルファベットのテキストを三つのアルファベットに圧縮するのはいいか、忘れちゃいけないものがあります。そう、辞書そのものを容量をかかります。もし辞書の容量も計算すると、なんと、１３個のアルファベットを圧縮するために、15個のアルファベットを消費しました。これは逆に容量を増えているではないか？でも、別にこれがBPE技術が実は無効というわけではなく、あくまでも、ウィキペディアの例が誤解しやすいだけです。例えば、元のテキストが二倍長さになる場合はどうなりますか：

![alt text](../pics/BPE3.svg)

同じ辞書を使う場合、別に取り扱うテキストの長さが倍にしても、辞書の長さは不変であることは自明です。同時に、辞書化した後のテキストも元のテキストと同じ二倍にすることになります。しかし、辞書化した後に、二倍にした部分は極端に圧縮したことがものが語っていって、全体の容量は１８しか消費していません。この場合、26個のアルファベットを消費するはずの場合、辞書化した後18しかつかっていないという状況になりました。もし、テキストがもっと長くなると、こういった圧縮の効果がより顕著になるでしょう。

それでは、実際に体験してみよう。これから、少々コードが出ます、分かりやすいように書いているのは、ご心配なく。



LLM透かしを説明する前に、とりあえずLLMそのものをしっかり理解する必要があります。理解といっても、別に自作LLMを作れるほどの理解を求めるわけではなく、あらすじを理解していただければ十分だと思います。


LLMとは、大規模言語モデル（Large Language Model）の略書です。その名前の通り、最初は言語そのものを対象にし、言語のパータンや性質などについてモデリングする手法です。近年（２０２０年以降）は、主要なＩＴ会社が競い合って、より大規模のモデルをいち早く作っているという形で、短期間でいくつかのＬＬＭサービスが世に出回っています。例えば、ＯｐｅｎＡＩのChatGPTや、グーグルのGeminiなど、一見多種多様なサービスが存在していますが、原理にたどれば、存外に一つ共通点があります。何を隠そう、まさにTransformerもですです。


[1]https://www.edu.i.hosei.ac.jp/~sigesada/kyouzai/mojicodehenkan.html

[2]Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019): 9.

[3]https://ja.wikipedia.org/wiki/Unicode

[4]https://www.unicode.org/versions/Unicode16.0.0/UnicodeStandard-16.0.pdf

[5]https://ja.wikipedia.org/wiki/バイト対符号化