# なぜLLMの文書を追跡する技術が必要とされるか？

２０２５年８月２６日、日経新聞および朝日新聞がアメリカのパープレキシティを提訴しました。日経・朝日側の論点は著作権侵害でした。実はこれは最初のAIサービスの中に著作権にめぐる論争ではなく、２０２５年半ばまでには、アメリカではすでに二十件以上の訴訟が記録されていました。それでは、なぜこういった新聞社がAIに論争を繰り広げられるのか？原因は、新聞記事のデータがよりいいAIサービスのために適しているからでした。

![alt text](RAG.svg)

例えば、上の図のように、ユーザーが同じ質問を二つのAIに投げるとします。この際、AI太郎は単なる答えを生成するに対して、AI次郎はとりあえず、この問題をグーグルで検索し、それから自社で集めたデータベースで照合し、最終的には人間の専門家に頼んでチェックを入れました後に、答えを出します。普通に考えたら、AI次郎の答えの方が絶対AI太郎のそれより優れたものに違いありません。しかし、グーグルはともかく、専門家を雇って質問ごとにチェックを入れるのは現実ではありません。今最も採用されている手法として、より多い信頼性のあるデータを自社のデータベースの中に詰め込むことにより、文書を生成する際の信頼性をある程度担保するというやり方です。

この場合、先説明した日経新聞の話に繋がります。新聞というのは、新聞者が入念にファクトチェックをした高い信頼性をもつデータです。それを参照用データベースにとってはもってこいなんですが、何せ著作権のあるもので、安易に使うのはできません。かといって、もしそれが使われた場合、別にAI会社がそれを狙って新聞データを盗むような真似をしたわけではなく、あくまでもデータベースの中身が膨大過ぎて、開発者すらすべてのデータを把握することは難しいでしょう。その上、現代のＬＬＭを訓練するために、安直にして、自社のデータベースだけを使うのはできません。往々にして、とりあえずインターネット上のありとあらゆるデータを一通り学習し、ＬＬＭの中に言語の基礎知識をある程度蓄積して、そこから、高い質の自社データベースを投入し、色々の機能を付け加えます。もちろん、新聞記事もインターネット上のデータの一つで、例えあえてその新聞社のウェブサイト避けるように学習にしても、その記事がインターネットで回って、どこか別のところでＬＬＭに拾われる可能性もあります。結論として、現代のＬＬＭの訓練にあまたのデータは必要です。しかし、そのデータの中に著作権がついているデータが存在しているかどうかを判断する手法は、いまだ確立されておりません。

ＬＬＭのデータを追跡し、もしくは判断するのは、別にデータを持っている人だけ気になるものではなく、ＬＬＭの開発元、例えばOpenAIやグーグルなどのIT会社にも、切羽詰まっている問題になりつつあります。先説明した通り、AIを作るために、膨大な高品質のデータが必要とされています。しかし、生成AIがこれまで普及したら、それにより生成した文章も世にはびこるわけです。そこで、もしAIを学習に、他のAIが生成したデータが取り入れたらどうなるか、説明するまでもなく、極めて悪い影響が与えます「２，３」。これはウロボロス現象とも呼べる事象で、それが発見されたらホムンクルスになって錬金術師とバトルする、わけでなく、モデルの性能が著しく低下する傾向が見られます。それを防ぐために、自分のAIの生成した内容を追跡し、それをもう一度に自分の訓練用のデータセットに入らないようにするのは重要であることが言うまでもなく、それをある程度実現した手法は２０２５年の雑誌Natureにも掲載されました。

これまで説明した内容は、別にパンピーとよっぽど関係がないかもしれないが、実はAIの追跡は普通のユーザーにとっても極めて重要な話題です。最近AIを使ってチートする学生など、よく新聞で出で来るではないか、それと伴い、宿題などの中に、AIが生成した部分があるかどうかを判断するサービスも近年増えてきました。しかし、それの信頼性はまだ未知数です。第一、AIの文章をどう判断するのはいくつかの手法がありますが、どのように最善を尽くすのは今定説はまだありません。それより、どうやって人間が書いていたものを、絶対AIに判断しないというのは、より重要な課題であると考えております。冤罪をかぶせるようなことを一度もしたら、根本的にサービスへの信頼が崩されるからです。近い未来、皆さん多分こういうAI検出サービスに出くわすかもしれないし、学生の方は多分すでに経験済みだと思います。どうやってより適切に判断を下し、同時に極力誤った検出をしないことは、皆さんの卒業（最近卒論の中にAIがあるかどうかを判断する技術を使う学校もありました）やこれからのAIの使用に関しては、極めて重要な話になると思います。

それでは、話を進めますが、なぜ別の技術ではなく、ＬＬＭ透かしにこんなに注力するのか？これは、別の技術の根本的なところが、先説明した、絶対に誤った判断をしないと相反することを感じているからです。極めて大まかに説明すると、今よく使われている代表的な手法は主に二種類あります。検出専用のプログラミングを作るのか[4]、まだは一層ＬＬＭを使って与えられた文章がＬＬＭにより生成されし文章かどうかを判断します[5]。

前者は大量にＬＬＭの生成したデータを集めて、その上で、何かＬＬＭ文章にあるでも人間が書いた文書にはない特徴でも探し、それを根拠に両者を見分けます。もちろん、分類においては、機械学習がよくつかわれるから、もちろん分類モデルを学習させるともできます[5]。しかし、このような手法なら、予め集めたデータに多い影響を受けるのは一つ重要な欠点であり、何よりＬＬＭの進歩がすさまじく、今日見つかったＬＬＭの文書の特徴が、明日になったらなくなるかもしれません。そこで、もう一つ種類の手法が提案されました、ざっくりいうと、ＬＬＭの能力そこまでなら、ＬＬＭそのものを使って、ＬＬＭ文書を判断するのも可能のはずだという手法です。例えば、ＬＬＭにそもまま判断すべき文書を投げて、これが人間が書いたかどうかを直球で聞いてみます。もっと巧みに、OUTFOX[5]という手法があって、ＬＬＭに同時に参考になれる人間が書いた文書とＬＬＭが書いた文書を上げて、それと一個所属不明の文書を与えする。この所属不明な文書が一体どのグループに入れるのか、LLMに聞いてみます。OUTFOX[5]は筆者と同じ大学の研究者が出していた論文ですが、加担なきでも創造力のある手法だと言い切れます。しかし、こういうＬＬＭが書いた文書かどうかの判断をＬＬＭそのものに任せるとかというのは疑問が残しています。言い換えれば、論理的にタコ足配線になってしまいます。ＬＬＭ文書の判断の信頼性をより理論的の信頼性がまだ証明されていない、ＬＬＭそのものの信頼性に託しています。

こういった諸々の問題を解決したく、ＬＬＭ電子透かしがいよいよ登場します。しかし、次の一節に足を踏み入れる前に、これまでに話したことを少々まとまります。

------
近年LLMが社会にあまたなインパクトを与えていて、それを追跡し、真偽を判断する技術の重要性も日に日に顕著になりました。この技術は、新聞社などの知的財産権などを保有している会社だけではなく、ＡＩの開発者、一人ひとり個人のユーザーにおいても重要な話ななりえます。いくつかの技術がすでに提案され、こういう状態を改善しようとするが、何せ信頼性がまだ不足しています。そこで、ＬＬＭ透かしが脚光を浴びたというわけです。

-----

https://www.nikkei.com/article/DGXZQOUD224SD0S5A720C2000000/


[2]Alemohammad, Sina, et al. "Self-consuming generative models go mad." The Twelfth International Conference on Learning Representations. 2023.

[3]Shumailov, Ilia, et al. "The curse of recursion: Training on generated data makes models forget." arXiv preprint arXiv:2305.17493 (2023).

[4]Guo, Biyang, et al. "How close is chatgpt to human experts? comparison corpus, evaluation, and detection." arXiv preprint arXiv:2301.07597 (2023).

[5]Koike, Ryuto, Masahiro Kaneko, and Naoaki Okazaki. "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024.
