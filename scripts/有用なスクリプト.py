import math, os
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers
from loguru import logger

def train_tokenizer(file_path: str | list[str], vocab_size = 5000, text_to_compress = "å¾è¼©ã‚ãŒã¯ã„ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚", silent_train = False) -> Tokenizer:

    if isinstance(file_path, str):
        assert os.path.exists(file_path), f"è¨“ç·´ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    elif isinstance(file_path, list):
        for file in file_path:  assert os.path.exists(file), f"è¨“ç·´ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    else: raise
    
    VOCAB_SIZE = vocab_size # è¾æ›¸ã®å¤§ãã•
    
    # åˆæœŸåŒ–
    tokenizer = Tokenizer(models.BPE())  # huggingface ã®å…¬å¼BPEå®Ÿè£…ã‚’ä½¿ã„ã¾ã™
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)

    # Train the tokenizer
    print(f"Tokenizerã®æŒ‡å®šå˜èªæ•°ã¯ {VOCAB_SIZE}...")
    if isinstance(file_path, str): 
        tokenizer.train(files=[file_path], trainer=trainer)
    else:
        tokenizer.train(files=file_path, trainer=trainer)
    print("è¨“ç·´å®Œäº†")
    if silent_train: return tokenizer

    # --- 3. æ¯”è¼ƒç”¨ã‚µãƒ³ãƒ—ãƒ«ã§è©¦ã™ ---
    print()
    print(f"--- ãƒ†ã‚­ã‚¹ãƒˆå®¹é‡æ¯”è¼ƒ ---")
    print(f"æ¯”è¼ƒç”¨ã‚µãƒ³ãƒ—ãƒ«: '{text_to_compress}'")

    # --- 4.  'Unicode' (UTF-8)  ---
    # ã¨ã‚Šã‚ãˆãšUTFï¼˜ã®å®¹é‡ã‚’è¨ˆç®—ã™ã‚‹
    utf8_bytes = text_to_compress.encode('utf-8')
    utf8_bit_count = len(utf8_bytes) * 8 # ï¼‘Bytes ã¯ ï¼˜bits
    
    print()
    print(f"[UTF-8 å®¹é‡]")
    print(f"  bytes: {len(utf8_bytes)}")
    print(f"  bits (bytes * 8): {utf8_bit_count}")

    # --- 5. Calculate BPE Bits ---
    # Encode the text using our trained BPE model
    encoding = tokenizer.encode(text_to_compress)
    bpe_tokens = encoding.ids

    # The number of bits per token is determined by the vocabulary size.
    # We need log2(vocab_size) bits to represent any token ID.
    actual_vocab_size = tokenizer.get_vocab_size()
    bits_per_token = math.ceil(math.log2(actual_vocab_size)) 

    # Total bits = number of tokens * bits per token
    total_bpe_bits = len(bpe_tokens) * bits_per_token
    
    print()
    print(f"[BPE å®¹é‡]")
    print(f"  è¾æ›¸å®¹é‡: {actual_vocab_size}")
    print(f"  ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®bitså¯†åº¦ (ceil(log2(vocab_size))): {bits_per_token}")
    print(f"  ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(bpe_tokens)}")
    print(f"  ãƒˆãƒ¼ã‚¯ãƒ³: {bpe_tokens}")
    print(f"  bits (ãƒˆãƒ¼ã‚¯ãƒ³æ•° * bits_per_token): {total_bpe_bits}")

    print()
    print("--- ğŸ“Š æœ€çµ‚çµæœ ---")
    print(f"UTF-8 Bits: {utf8_bit_count}")
    print(f"BPE Bits:   {total_bpe_bits}")

    compression_ratio = utf8_bit_count / total_bpe_bits
    print(f"-> åœ§ç¸®æ¯”ç‡: {compression_ratio:.2f} (UTF-8 bits / BPE bits)")
    
    return tokenizer

def load_txt(path="./scripts/å¾è¼©ã¯çŒ«ã§ã‚ã‚‹utf8.txt") -> list[str]:
    with open(path, encoding="utf8") as f:
        lines = f.readlines()
        
    return lines