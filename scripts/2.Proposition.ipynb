{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40d384a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹\n",
      "\n",
      "å¤ç›®æ¼±çŸ³\n",
      "\n",
      "ã€€å¾è¼©ã€Šã‚ãŒã¯ã„ã€‹ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚\n",
      "\n",
      "ã€€ã©ã“ã§ç”Ÿã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ã€Šã‘ã‚“ã¨ã†ã€‹ãŒã¤ã‹ã¬ã€‚ä½•ã§ã‚‚è–„æš—ã„ã˜ã‚ã˜ã‚ã—ãŸæ‰€ã§ãƒ‹ãƒ£ãƒ¼ãƒ‹ãƒ£ãƒ¼æ³£ã„ã¦ã„ãŸäº‹ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n",
      "ã€€ã“ã®æ›¸ç”Ÿã®æŒã®è£ã€Šã†ã¡ã€‹ã§ã—ã°ã‚‰ãã¯ã‚ˆã„å¿ƒæŒã«åã£ã¦ãŠã£ãŸãŒã€ã—ã°ã‚‰ãã™ã‚‹ã¨éå¸¸ãªé€ŸåŠ›ã§é‹è»¢ã—å§‹ã‚ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n",
      "ã€€ãµã¨æ°—ãŒä»˜ã„ã¦è¦‹ã‚‹ã¨æ›¸ç”Ÿã¯ã„ãªã„ã€‚ãŸãã•ã‚“ãŠã£ãŸå…„å¼ŸãŒä¸€ï½œç–‹ã€Šã´ãã€‹ã‚‚è¦‹ãˆã¬ã€‚è‚å¿ƒã€Šã‹ã‚“ã˜ã‚“ã€‹ã®æ¯ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n",
      "ã€€ã‚ˆã†ã‚„ãã®æ€ã„ã§ç¬¹åŸã‚’é€™ã„å‡ºã™ã¨å‘ã†ã«å¤§ããªæ± ãŒã‚ã‚‹ã€‚å¾è¼©ã¯æ± ã®å‰ã«åã£ã¦ã©ã†ã—ãŸã‚‰ã‚ˆã‹ã‚ã†ã¨è€ƒãˆã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n",
      "ã€€å¾è¼©ã®ä¸»äººã¯æ»…å¤šã€Šã‚ã£ãŸã€‹ã«å¾è¼©ã¨é¡”ã‚’åˆã›ã‚‹äº‹ãŒãªã„ã€‚è·æ¥­ã¯æ•™å¸«ã ãã†ã ã€‚å­¦æ ¡ã‹ã‚‰å¸°ã‚‹ã¨çµ‚æ—¥æ›¸æ–ã«ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n",
      "ã€€å¾è¼©ãŒã“ã®å®¶ã¸ä½ã¿è¾¼ã‚“ã å½“æ™‚ã¯ã€ä¸»äººä»¥å¤–ã®ã‚‚ã®ã«ã¯ã¯ãªã¯ã ä¸äººæœ›ã§ã‚ã£ãŸã€‚ã©ã“ã¸è¡Œã£ã¦ã‚‚è·³ã€Šã¯ã€‹ã­ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\n"
     ]
    }
   ],
   "source": [
    "from æœ‰ç”¨ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ import load_txt\n",
    "for sentence in load_txt(path=\"./å¾è¼©ã¯çŒ«ã§ã‚ã‚‹utf8.txt\")[:10]:\n",
    "    if len(sentence) > 1:\n",
    "        print(sentence if len(sentence) < 50 else sentence[:50]+\"ã€€ï¼ˆä»¥ä¸‹ç•¥ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d54c4c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizerã®æŒ‡å®šå˜èªæ•°ã¯ 128...\n",
      "è¨“ç·´å®Œäº†\n",
      "\n",
      "--- ãƒ†ã‚­ã‚¹ãƒˆå®¹é‡æ¯”è¼ƒ ---\n",
      "æ¯”è¼ƒç”¨ã‚µãƒ³ãƒ—ãƒ«: 'å¾è¼©ã‚ãŒã¯ã„ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚'\n",
      "\n",
      "[UTF-8 å®¹é‡]\n",
      "  bytes: 60\n",
      "  bits (bytes * 8): 480\n",
      "\n",
      "[BPE å®¹é‡]\n",
      "  è¾æ›¸å®¹é‡: 128\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®bitså¯†åº¦ (ceil(log2(vocab_size))): 7\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 60\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³: [85, 110, 78, 88, 76, 58, 83, 96, 109, 83, 95, 106, 83, 95, 63, 83, 95, 98, 83, 95, 63, 87, 106, 60, 83, 95, 56, 83, 95, 96, 83, 96, 105, 83, 94, 96, 85, 110, 107, 85, 103, 107, 83, 95, 63, 83, 95, 78, 83, 95, 126, 87, 98, 50, 83, 95, 98, 83, 94, 96]\n",
      "  bits (ãƒˆãƒ¼ã‚¯ãƒ³æ•° * bits_per_token): 420\n",
      "\n",
      "--- ğŸ“Š æœ€çµ‚çµæœ ---\n",
      "UTF-8 Bits: 480\n",
      "BPE Bits:   420\n",
      "-> åœ§ç¸®æ¯”ç‡: 1.14 (UTF-8 bits / BPE bits)\n"
     ]
    }
   ],
   "source": [
    "from æœ‰ç”¨ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ import train_tokenizer\n",
    "\n",
    "tokenizer = train_tokenizer(\n",
    "    file_path=\"./å¾è¼©ã¯çŒ«ã§ã‚ã‚‹utf8.txt\",   # ãƒ†ã‚­ã‚¹ãƒˆå…ƒã«ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    vocab_size=128,                       # è¾æ›¸ã®å¤§ãã•\n",
    "    text_to_compress = \"å¾è¼©ã‚ãŒã¯ã„ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8467bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from æœ‰ç”¨ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ import load_txt\n",
    "\n",
    "for sentence in load_txt(path=\"./å¾è¼©ã¯çŒ«ã§ã‚ã‚‹utf8.txt\"):\n",
    "    assert tokenizer.decode(tokenizer.encode(sentence).ids) == sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29769fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é©å½“ãªæ–‡ç« :\t è«–æ–‡ã‚’æ›¸ãã®ã¯è¾›ã„ El Psy Congroo ğŸ¥º\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³: \t [88, 60, 116, 86, 116, 101, 83, 96, 112, 86, 121, 72, 83, 95, 109]\n",
      "å¾©å…ƒã—ãŸæ—¥æœ¬èª:\t è«–æ–‡ã‚’æ›¸ãã®ã¯è¾›ã„ l sy ongroo ï¿½ï¿½ï¿½\n"
     ]
    }
   ],
   "source": [
    "é©å½“ãªæ–‡ç«  = \"è«–æ–‡ã‚’æ›¸ãã®ã¯è¾›ã„ El Psy Congroo ğŸ¥º\"\n",
    "print(\"é©å½“ãªæ–‡ç« :\\t\", é©å½“ãªæ–‡ç« )\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³ = tokenizer.encode(é©å½“ãªæ–‡ç« ).ids\n",
    "print(\"ãƒˆãƒ¼ã‚¯ãƒ³: \\t\", ãƒˆãƒ¼ã‚¯ãƒ³[:15])\n",
    "æ—¥æœ¬èªæ–‡æ›¸ = tokenizer.decode(ãƒˆãƒ¼ã‚¯ãƒ³)\n",
    "print(\"å¾©å…ƒã—ãŸæ—¥æœ¬èª:\\t\", æ—¥æœ¬èªæ–‡æ›¸)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "355e1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizerã®æŒ‡å®šå˜èªæ•°ã¯ 5000...\n",
      "è¨“ç·´å®Œäº†\n",
      "\n",
      "--- ãƒ†ã‚­ã‚¹ãƒˆå®¹é‡æ¯”è¼ƒ ---\n",
      "æ¯”è¼ƒç”¨ã‚µãƒ³ãƒ—ãƒ«: 'å¾è¼©ã‚ãŒã¯ã„ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚'\n",
      "\n",
      "[UTF-8 å®¹é‡]\n",
      "  bytes: 60\n",
      "  bits (bytes * 8): 480\n",
      "\n",
      "[BPE å®¹é‡]\n",
      "  è¾æ›¸å®¹é‡: 5000\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®bitså¯†åº¦ (ceil(log2(vocab_size))): 13\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 13\n",
      "  ãƒˆãƒ¼ã‚¯ãƒ³: [460, 3696, 813, 167, 808, 324, 164, 2836, 167, 1158, 566, 151, 164]\n",
      "  bits (ãƒˆãƒ¼ã‚¯ãƒ³æ•° * bits_per_token): 169\n",
      "\n",
      "--- ğŸ“Š æœ€çµ‚çµæœ ---\n",
      "UTF-8 Bits: 480\n",
      "BPE Bits:   169\n",
      "-> åœ§ç¸®æ¯”ç‡: 2.84 (UTF-8 bits / BPE bits)\n",
      "----------------------------\n",
      "é©å½“ãªæ–‡ç« :\t æœˆãŒãã‚Œã„ã§ã™ El Psy Congroo ğŸ¥º\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³: \t [493, 170, 230, 199, 151, 314, 112, 26, 51, 112, 32, 58, 64, 300, 46]\n",
      "å¾©å…ƒã—ãŸæ—¥æœ¬èª:\t æœˆãŒãã‚Œã„ã§ã™ El Psy ongroo ğŸ¥º\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = train_tokenizer(\n",
    "    file_path=[         # ãƒ†ã‚­ã‚¹ãƒˆå…ƒã«ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "        \"./å¾è¼©ã¯çŒ«ã§ã‚ã‚‹utf8.txt\",\n",
    "        \"./text/emoji.txt\",\n",
    "        \"./text/harry potter 1.txt\"\n",
    "    ],                  \n",
    "    vocab_size=5000,    # è¾æ›¸ã®å¤§ãã•\n",
    "    text_to_compress = \"å¾è¼©ã‚ãŒã¯ã„ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚\"\n",
    ")\n",
    "print(\"----------------------------\")\n",
    "é©å½“ãªæ–‡ç«  = \"æœˆãŒãã‚Œã„ã§ã™ El Psy Congroo ğŸ¥º\"\n",
    "print(\"é©å½“ãªæ–‡ç« :\\t\", é©å½“ãªæ–‡ç« )\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³ = tokenizer.encode(é©å½“ãªæ–‡ç« ).ids\n",
    "print(\"ãƒˆãƒ¼ã‚¯ãƒ³: \\t\", ãƒˆãƒ¼ã‚¯ãƒ³[:15])\n",
    "æ—¥æœ¬èªæ–‡æ›¸ = tokenizer.decode(ãƒˆãƒ¼ã‚¯ãƒ³)\n",
    "print(\"å¾©å…ƒã—ãŸæ—¥æœ¬èª:\\t\", æ—¥æœ¬èªæ–‡æ›¸)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa576cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‘ = [1, 2, 3, 4, 5, ...]\n",
    "ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼’ = [1, 4, 9, 16, 25, ...]\n",
    "ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼“ = [1, 2, 4, 8, 16, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec4d0be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‘ -> [1, 2, 3, 4, 5, 126]\n"
     ]
    }
   ],
   "source": [
    "def ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‘(x: int):\n",
    "    # ï½˜ãŒï¼‘ã‹ã‚‰ï¼•ã®æ™‚ã€ï½˜ã—ã‹å€¤ã¨ã—ã¦æˆ»ã£ã¦ã„ãªã„ï¼›\n",
    "    return x + (x - 1) * (x - 2) * (x - 3) * (x - 4) * (x - 5) \n",
    "\n",
    "\n",
    "æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‘ = [ãƒ‘ã‚¿ãƒ¼ãƒ³1(x) for x in range(1, 7)]\n",
    "print(\"æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‘ ->\", æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã€€ã€€ã€€ã€€ã€€ æœˆ : [493]\n",
      "ã€€ã€€ã€€ã€€ æœˆãŒ : [493, 170]\n",
      "ã€€ã€€ã€€ æœˆãŒã : [493, 170, 230]\n",
      "ã€€ã€€ æœˆãŒãã‚Œ : [493, 170, 230, 199]\n",
      "ã€€ æœˆãŒãã‚Œã„ : [493, 170, 230, 199, 151]\n"
     ]
    }
   ],
   "source": [
    "æ–‡æ›¸ = \"æœˆãŒãã‚Œã„\"\n",
    "for idx in range(0, len(æ–‡æ›¸)):\n",
    "    æ–‡æ›¸æ–­ç‰‡ = æ–‡æ›¸[: idx + 1]\n",
    "    tokens = tokenizer.encode(æ–‡æ›¸æ–­ç‰‡)\n",
    "    print(\"ã€€\" * (len(æ–‡æ›¸) - idx), æ–‡æ›¸æ–­ç‰‡, \":\", end=\" \")\n",
    "    print(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4af4f5988b458c9786e6c0ffba36a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xhaug\\Playground\\ï¼£ï¼­ï¼‘ï¼ï¼—\\Easy_LLM_Watermark\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xhaug\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-pt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd61848daa14fa5a82e06ad9870ef3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the heart of Paris, France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611fbbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¦–éƒ½\n"
     ]
    }
   ],
   "source": [
    "prompt = \"æ±äº¬ã¯æ—¥æœ¬ã®\"\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=1, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b984e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4242513369.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprompt = \"æ±äº¬ã¯æ—¥æœ¬ã®\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "prompt = \"æ±äº¬ã¯æ—¥æœ¬ã®\"\n",
    "max_new_token = 13\n",
    "for i in range(max_new_token):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**model_inputs, max_new_tokens=1, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "\n",
    "    decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
    "    print(prompt+decoded)\n",
    "    prompt +=decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easy-llm-watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
