{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d66476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8cc25a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は猫である\n",
      "\n",
      "夏目漱石\n",
      "\n",
      "\n",
      "\n",
      "　吾輩《わがはい》は猫である。名前はまだ無い。\n",
      "\n",
      "　どこで生れたかとんと見当《けんとう》がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事　（以下略）\n",
      "　この書生の掌の裏《うち》でしばらくはよい心持に坐っておったが、しばらくすると非常な速力で運転し始め　（以下略）\n",
      "　ふと気が付いて見ると書生はいない。たくさんおった兄弟が一｜疋《ぴき》も見えぬ。肝心《かんじん》の母　（以下略）\n",
      "　ようやくの思いで笹原を這い出すと向うに大きな池がある。吾輩は池の前に坐ってどうしたらよかろうと考え　（以下略）\n",
      "　吾輩の主人は滅多《めった》に吾輩と顔を合せる事がない。職業は教師だそうだ。学校から帰ると終日書斎に　（以下略）\n",
      "　吾輩がこの家へ住み込んだ当時は、主人以外のものにははなはだ不人望であった。どこへ行っても跳《は》ね　（以下略）\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from 有用なスクリプト import load_吾輩は猫である\n",
    "for sentence in load_吾輩は猫である(path=\"./吾輩は猫であるutf8.txt\")[:10]:\n",
    "    print(sentence if len(sentence) < 50 else sentence[:50]+\"　（以下略）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b003326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer with vocab size 5000...\n",
      "Training complete.\n",
      "\n",
      "--- Comparing Bits for Text ---\n",
      "Target Text: '吾輩わがはいは猫である。名前はまだ無い。'\n",
      "\n",
      "[UTF-8 Calculation]\n",
      "  Total bytes: 60\n",
      "  Total bits (bytes * 8): 480\n",
      "\n",
      "[BPE Calculation]\n",
      "  Vocabulary size: 5000\n",
      "  Bits needed per token (ceil(log2(vocab_size))): 13\n",
      "  Number of tokens: 10\n",
      "  BPE Tokens: [315, 2459, 535, 144, 4662, 142, 1873, 3784, 3646, 142]\n",
      "  Total bits (tokens * bits_per_token): 130\n",
      "\n",
      "--- 📊 Final Result ---\n",
      "UTF-8 Bits: 480\n",
      "BPE Bits:   130\n",
      "\n",
      "Compression Ratio: 3.69 (UTF-8 bits / BPE bits)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# VOCAB_SIZE = 5000 # 辞書の大きさ\n",
    "VOCAB_SIZE = 5000 # 辞書の大きさ\n",
    "\n",
    "# 初期化\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Train the tokenizer\n",
    "print(f\"Training BPE tokenizer with vocab size {VOCAB_SIZE}...\")\n",
    "tokenizer.train(files=[\"./吾輩は猫であるutf8.txt\"], trainer=trainer)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 3. Target Text to Compress ---\n",
    "text_to_compress = \"吾輩わがはいは猫である。名前はまだ無い。\"\n",
    "\n",
    "print(f\"\\n--- Comparing Bits for Text ---\")\n",
    "print(f\"Target Text: '{text_to_compress}'\")\n",
    "\n",
    "# --- 4. Calculate 'Unicode' (UTF-8) Bits ---\n",
    "# We encode the string into bytes using UTF-8\n",
    "utf8_bytes = text_to_compress.encode('utf-8')\n",
    "utf8_bit_count = len(utf8_bytes) * 8\n",
    "\n",
    "print(f\"\\n[UTF-8 Calculation]\")\n",
    "print(f\"  Total bytes: {len(utf8_bytes)}\")\n",
    "print(f\"  Total bits (bytes * 8): {utf8_bit_count}\")\n",
    "\n",
    "# --- 5. Calculate BPE Bits ---\n",
    "# Encode the text using our trained BPE model\n",
    "encoding = tokenizer.encode(text_to_compress)\n",
    "bpe_tokens = encoding.ids\n",
    "\n",
    "# The number of bits per token is determined by the vocabulary size.\n",
    "# We need log2(vocab_size) bits to represent any token ID.\n",
    "actual_vocab_size = tokenizer.get_vocab_size()\n",
    "bits_per_token = math.ceil(math.log2(actual_vocab_size)) \n",
    "\n",
    "# Total bits = number of tokens * bits per token\n",
    "total_bpe_bits = len(bpe_tokens) * bits_per_token\n",
    "\n",
    "print(f\"\\n[BPE Calculation]\")\n",
    "print(f\"  Vocabulary size: {actual_vocab_size}\")\n",
    "print(f\"  Bits needed per token (ceil(log2(vocab_size))): {bits_per_token}\")\n",
    "print(f\"  Number of tokens: {len(bpe_tokens)}\")\n",
    "print(f\"  BPE Tokens: {bpe_tokens}\")\n",
    "print(f\"  Total bits (tokens * bits_per_token): {total_bpe_bits}\")\n",
    "\n",
    "# --- 6. Final Comparison ---\n",
    "print(\"\\n--- 📊 Final Result ---\")\n",
    "print(f\"UTF-8 Bits: {utf8_bit_count}\")\n",
    "print(f\"BPE Bits:   {total_bpe_bits}\")\n",
    "\n",
    "compression_ratio = utf8_bit_count / total_bpe_bits\n",
    "print(f\"\\nCompression Ratio: {compression_ratio:.2f} (UTF-8 bits / BPE bits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf753ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吾輩わがはいは猫である。名前はまだ無い。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easy-llm-watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
